{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\sally\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\sally\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\sally\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\sally\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\sally\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\sally\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import backend as BK\n",
    "import cv2\n",
    "from scipy.io.wavfile import read\n",
    "from scipy.io.wavfile import write\n",
    "import scipy.io as sio\n",
    "from collections import OrderedDict\n",
    "import random \n",
    "random.seed(100)\n",
    "# %pylab inline\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Define variables and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "word = 'z'\n",
    "\n",
    "num_videos = 38\n",
    "\n",
    "num_test=10\n",
    "\n",
    "\n",
    "num_train=num_videos-num_test\n",
    "num_img =126\n",
    "img_width,img_height,num_slices =28,28,2\n",
    "slice_length = int(num_img/num_slices)\n",
    "print(slice_length)\n",
    "\n",
    "\n",
    "base_path='D:/Mrs_backup/speech_test/all_vocabulary/gp_model/gp/'+ word + '/'\n",
    "audio_path ='D:/Mrs_backup/speech_test/all_vocabulary/gp_model/audio_spec/'+ word + '/'\n",
    "autoencoder_path= 'D:/Mrs_backup/speech_test/all_vocabulary/04_autoencoder/'+ word + '/'\n",
    "specTxtfile='D:/Mrs_backup/speech_test/all_vocabulary/gp_model/spec_txt/'+ word+'_valid_aud_specs.txt'\n",
    "\n",
    "integrate_data_path ='D:/Mrs_backup/speech_test/all_vocabulary/gp_model/modelInput/'+ word + '/'\n",
    "if not os.path.exists(integrate_data_path):\n",
    "    os.mkdir(integrate_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Define functions for video input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define diff function to add 3 time-derivative channels\n",
    "def diff(buf_input):\n",
    "    buf_input=np.pad(buf_input,((0,0),(0,0),(1,0)),'edge')\n",
    "    buf_output=np.diff(buf_input,axis=2)\n",
    "    #print(buf_output.shape)\n",
    "    return buf_output\n",
    "\n",
    "# Define slice_video_3D function to divide into 15 non-overlap slices each of length 5\n",
    "def slice_video_3D(video):\n",
    "    video_output =np.empty((num_slices,3,img_height,img_width,slice_length), np.dtype('float32'))\n",
    "    \n",
    "    start=0\n",
    "    for i in range(0,num_slices):\n",
    "        video_output[i,:,:,:,:]=video[:,:,:,start:start+slice_length]\n",
    "        start+=slice_length\n",
    "    return video_output\n",
    "\n",
    "# Define diff function to add 3 time-derivative channels\n",
    "def diff(buf_input):\n",
    "    buf_input=np.pad(buf_input,((0,0),(0,0),(1,0)),'edge')\n",
    "    buf_output=np.diff(buf_input,axis=2)\n",
    "    #print(buf_output.shape)\n",
    "    return buf_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76, 3, 28, 28, 63)\n"
     ]
    }
   ],
   "source": [
    "# Define the shape of the video_input [10*15,40,80,5]\n",
    "video_input =np.empty((num_videos*(num_slices),3,int(img_height),int(img_width),int(slice_length)), np.dtype('float32'))\n",
    "print(video_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #D:\\speech_dataset\\GRID\\GP_100_75frames\\s1_gp_test\\gabor\\bbal8p\n",
    "# img_path = base_path +'002/27.jpg'\n",
    "# img = mpimg.imread(img_path)\n",
    "# imgplot = plt.imshow(img)\n",
    "# plt.show()\n",
    "\n",
    "# image = cv2.resize(img,(80,40))\n",
    "# print(image)\n",
    "# plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 Video slices shape:(76, 3, 28, 28, 63)\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "print(num_videos)\n",
    "for n in range(0,num_videos):\n",
    "    print(n, end=' ')\n",
    "    format_num1=\"{number:03}\".format(number=n)\n",
    "    path = base_path+str(format_num1)\n",
    "    dirs = os.listdir(path)\n",
    "    num_img = len(dirs) \n",
    "    #print(num_img)\n",
    "    \n",
    "    tmp0 = np.empty((int(img_height),int(img_width),int(num_img)), np.dtype('float32'))\n",
    "   \n",
    "    for i in range(0,num_img):\n",
    "        img_path =  path + '/'+str(dirs[i])\n",
    "        #print(img_path)\n",
    "        \n",
    "        \n",
    "        # Load an color image in grayscale\n",
    "        img = cv2.imread(img_path,0)\n",
    "        \n",
    "#         image = cv2.resize(img,(80,40))\n",
    "        image = cv2.resize(img,(28,28))\n",
    "        #print(image.shape)\n",
    "        \n",
    "        tmp0[:,:,i] = image\n",
    "        #print(tmp0)\n",
    "    \n",
    "    # Call diff function to add 3 time-derivative channels\n",
    "    diff_video=np.empty((3,tmp0.shape[0],tmp0.shape[1],tmp0.shape[2]))\n",
    "    diff_video[0,:,:,:]=tmp0\n",
    "    diff_video[1,:,:,:]=diff(tmp0)\n",
    "    diff_video[2,:,:,:]=diff(diff_video[1,:,:,:])\n",
    "    #print(diff_video.shape)\n",
    "    \n",
    "    # Call slice_video_3D function\n",
    "    data_vid=slice_video_3D(diff_video)\n",
    "    #data_vid=data_vid/255\n",
    "    #print(data_vid)\n",
    "    \n",
    "    # Add total number of slices \n",
    "    video_input[start:start+num_slices,:,:,:,:]=data_vid\n",
    "    #print(video_input.shape)\n",
    "    start+=num_slices\n",
    "    #print(start)\n",
    "    \n",
    "print('Video slices shape:'+str(video_input.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(video_input.shape)\n",
    "# print(video_input[100,0,:,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) Define functions for audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define slice_audio_spec function to to divide into 15 non-overlap slices each of length 26\n",
    "def slice_audio_spec(audio_spec):\n",
    "    global AUDIO_LENGTH\n",
    "    window_size=int(AUDIO_LENGTH/num_slices) #from time to number of audio index \n",
    "    #print(window_size)\n",
    "    audio_output =np.empty((num_slices,audio_spec.shape[0],window_size), np.dtype('float32'))\n",
    "    start=0\n",
    "    for i in range(0,num_slices):\n",
    "        audio_output[i,:,:]=audio_spec[:,start:start+window_size]\n",
    "        start+=window_size\n",
    "        if start>AUDIO_LENGTH-window_size:\n",
    "            break\n",
    "    #print(audio_output.shape)\n",
    "    #print(audio_output[1,:,1])\n",
    "    return audio_output\n",
    "\n",
    "# Define get activations function # Extract the 32-bin bottleneck features as target for the main network\n",
    "def get_activations(model, layer_in, layer_out, X_batch):\n",
    "    get_activations = BK.function([model.layers[layer_in].input, BK.learning_phase()], [model.layers[layer_out].output])\n",
    "    activations = get_activations([X_batch,0])\n",
    "    return activations\n",
    "\n",
    "#Define padding function\n",
    "def get_padded_spec(data):\n",
    "    \n",
    "    # Compress the spectrogram by raising to the power 1/3\n",
    "    #print(data[1:3,1])\n",
    "    data=np.power(data,.3)\n",
    "    #print(data[1,370])\n",
    "    \n",
    "    # Get the video length\n",
    "    t=data.shape[1]\n",
    "   \n",
    "    # Get the number of pads\n",
    "    num_pads=int((2*num_slices)-(t%(2*num_slices)))\n",
    "    #print(num_pads)\n",
    "    \n",
    "    # Add padding to the video length\n",
    "    padded_data=np.pad(data,((0,0),(0,num_pads)),'reflect')\n",
    "    #print(padded_data[1,370])\n",
    "    #print(padded_data[1,365:390])\n",
    "    #print(padded_data.shape)\n",
    "\n",
    "    return padded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) Load autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading autoencoder model...\n"
     ]
    }
   ],
   "source": [
    "# Define cost function [mean squared error + correlation loss]\n",
    "def corr2_mse_loss(a,b):\n",
    "    a = BK.tf.subtract(a, BK.tf.reduce_mean(a))\n",
    "    b = BK.tf.subtract(b, BK.tf.reduce_mean(b))\n",
    "    tmp1 = BK.tf.reduce_sum(BK.tf.multiply(a,a))\n",
    "    tmp2 = BK.tf.reduce_sum(BK.tf.multiply(b,b))\n",
    "    tmp3 = BK.tf.sqrt(BK.tf.multiply(tmp1,tmp2))\n",
    "    tmp4 = BK.tf.reduce_sum(BK.tf.multiply(a,b))\n",
    "    r = -BK.tf.divide(tmp4,tmp3)\n",
    "    m=BK.tf.reduce_mean(BK.tf.square(BK.tf.subtract(a, b)))\n",
    "    rm=BK.tf.add(r,m)\n",
    "    return rm\n",
    "\n",
    "# Load autoencoder model\n",
    "print('Loading autoencoder model...')\n",
    "config = BK.tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = BK.tf.Session(config=config)\n",
    "model=load_model(autoencoder_path+'autoencoder.h5',custom_objects={'corr2_mse_loss': corr2_mse_loss})\n",
    "model.load_weights(autoencoder_path+'autoencoder_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7)Audio feature extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio slices shape:(76, 32, 64)\n",
      "10/38\n",
      "20/38\n",
      "30/38\n",
      "Audio slices shape:(76, 32, 64)\n",
      "Target features to network shape:(76, 2048)\n"
     ]
    }
   ],
   "source": [
    "# Open the text file of auditory spectrogram input path \n",
    "text_file = open(specTxtfile,'r')\n",
    "\n",
    "# Load each line of the text file\n",
    "lines = text_file.read().split('\\n')\n",
    "index_shuf=list(range(len(lines)))\n",
    "lines_shuf=[]\n",
    "for i in index_shuf:\n",
    "    lines_shuf.append(lines[i])\n",
    "\n",
    "# Get the number of audios\n",
    "num_audios=len(lines)\n",
    "\n",
    "# Get data shape \n",
    "mat=sio.loadmat(lines[0])\n",
    "data = mat['y'].T[:,2:]\n",
    "\n",
    "# get_padded_spec function to get the shape after padding data\n",
    "padded_data=get_padded_spec(data=data)\n",
    "global AUDIO_LENGTH\n",
    "AUDIO_LENGTH=padded_data.shape[1]\n",
    "\n",
    "# Call get_activations function to get the shape after bottleneck features\n",
    "bottleneck=get_activations(model, 0, 12, padded_data.T)\n",
    "bottleneck=bottleneck[0].T\n",
    "\n",
    "# Get the total shape of audio_input variable\n",
    "audio_input =np.empty((num_audios*(num_slices),bottleneck.shape[0],int(AUDIO_LENGTH/num_slices)), np.dtype('float32'))\n",
    "print(\"Audio slices shape:\" + str(audio_input.shape))\n",
    "\n",
    "tmp =np.zeros((AUDIO_LENGTH), np.dtype('float32'))\n",
    "\n",
    "\n",
    "i=0\n",
    "for row in lines_shuf:\n",
    "    \n",
    "    # Load data from the path\n",
    "    mat=sio.loadmat(row)\n",
    "    \n",
    "    # Read data from the second feature\n",
    "    data = mat['y'].T[:,2:]\n",
    "    \n",
    "    # Call get_padded_spec function: \n",
    "    # (1). Compress the spectrogram by raising to the power 1/3 \n",
    "    # (2). Add padding to the video length \n",
    "    padded_data=get_padded_spec(data=data)\n",
    "   \n",
    "    \n",
    "    # Call get_activations function to get the bottleneck feature from autoencoder model \n",
    "    # Encoder auditory spectrogram\n",
    "    bottleneck=get_activations(model, 0, 12, padded_data.T)\n",
    "    \n",
    "    #Transpose bottleneck[0] varaible\n",
    "    bottleneck=bottleneck[0].T\n",
    "       \n",
    "    # Call slice_audio_spec function to divide into 15 non-overlap audio slices each of length [390/15] 26\n",
    "    data=slice_audio_spec(bottleneck)\n",
    "    #print(data.shape)\n",
    "    \n",
    "    #Get the total audio slices\n",
    "    audio_input[i*(num_slices):(i+1)*(num_slices),:,:]=data[:,:,:]\n",
    "    i+=1\n",
    "    if i>=num_audios:\n",
    "        break\n",
    "    if i%10==0:\n",
    "        print(str(i)+'/'+str(num_audios))\n",
    "\n",
    "audio_output=np.reshape(audio_input,(audio_input.shape[0],audio_input.shape[1]*audio_input.shape[2]))\n",
    "print('Audio slices shape:'+str(audio_input.shape))\n",
    "print('Target features to network shape:'+str(audio_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(8) Data_integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Saving data part1...\n",
      "Saving test data\n",
      "0 to 2\n",
      "Saving data part2...\n",
      "Saving test data\n",
      "2 to 4\n",
      "Saving data part3...\n",
      "Saving test data\n",
      "4 to 6\n",
      "Saving data part4...\n",
      "Saving test data\n",
      "6 to 8\n",
      "Saving data part5...\n",
      "Saving test data\n",
      "8 to 10\n",
      "Saving data part6...\n",
      "Saving test data\n",
      "10 to 12\n",
      "Saving data part7...\n",
      "Saving test data\n",
      "12 to 14\n",
      "Saving data part8...\n",
      "Saving test data\n",
      "14 to 16\n",
      "Saving data part9...\n",
      "Saving test data\n",
      "16 to 18\n",
      "Saving data part10...\n",
      "Saving test data\n",
      "18 to 20\n",
      "Saving data part11...\n",
      "Saving test data\n",
      "20 to 22\n",
      "Saving data part12...\n",
      "Saving test data\n",
      "22 to 24\n",
      "Saving data part13...\n",
      "Saving test data\n",
      "24 to 26\n",
      "Saving data part14...\n",
      "Saving test data\n",
      "26 to 28\n",
      "Saving data part15...\n",
      "Saving test data\n",
      "28 to 30\n",
      "Saving data part16...\n",
      "Saving test data\n",
      "30 to 32\n",
      "Saving data part17...\n",
      "Saving test data\n",
      "32 to 34\n",
      "Saving data part18...\n",
      "Saving test data\n",
      "34 to 36\n",
      "Saving data part19...\n",
      "Saving test data\n",
      "36 to 38\n",
      "Saving data part20...\n",
      "Saving test data\n",
      "38 to 40\n",
      "Saving data part21...\n",
      "Saving test data\n",
      "40 to 42\n",
      "Saving data part22...\n",
      "Saving test data\n",
      "42 to 44\n",
      "Saving data part23...\n",
      "Saving test data\n",
      "44 to 46\n",
      "Saving data part24...\n",
      "Saving test data\n",
      "46 to 48\n",
      "Saving data part25...\n",
      "Saving test data\n",
      "48 to 50\n",
      "Saving data part26...\n",
      "Saving test data\n",
      "50 to 52\n",
      "Saving data part27...\n",
      "Saving test data\n",
      "52 to 54\n",
      "Saving data part28...\n",
      "54 to 56\n",
      "Saving validation data...\n",
      "56 to 76\n",
      "(20, 3, 28, 28, 63)\n"
     ]
    }
   ],
   "source": [
    "N= num_train\n",
    "L=num_slices\n",
    "print(L)\n",
    "i=0\n",
    "for i in range(N):\n",
    "    if i<(N-1):\n",
    "        print('Saving data part'+str(i+1)+'...')\n",
    "        print('Saving test data')\n",
    "        start=i*L\n",
    "        end=(i+1)*L\n",
    "        print(str(start)+' to '+str(end))\n",
    "        sio.savemat(integrate_data_path+'preprocessed_data_final_part'+str(i+1)+'.mat', mdict={'video_input': video_input[start:end,:,:,:,:], 'audio_input' : audio_input[start:end,:,:]})\n",
    "    else:\n",
    "        print('Saving data part'+str(i+1)+'...')\n",
    "        start=i*L\n",
    "        end=num_train*num_slices\n",
    "        print(str(start)+' to '+str(end))\n",
    "        sio.savemat(integrate_data_path+'preprocessed_data_final_part'+str(i+1)+'.mat', mdict={'video_input': video_input[start:end,:,:,:,:], 'audio_input' : audio_input[start:end,:,:]})\n",
    "\n",
    "print('Saving validation data...')\n",
    "start=num_train*num_slices\n",
    "print(str(start)+' to '+str(video_input.shape[0]))\n",
    "print(video_input[start:,:,:,:,:].shape)\n",
    "sio.savemat(integrate_data_path+'preprocessed_data_final_validation.mat', mdict={'video_input': video_input[start:,:,:,:,:], 'audio_input' : audio_input[start:,:,:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
